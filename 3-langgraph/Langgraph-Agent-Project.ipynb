{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc871024",
   "metadata": {},
   "source": [
    "# Assignment 4: Multi-Agent System with Supervisor and Validation\n",
    "\n",
    "## Project Overview\n",
    "This project implements a multi-agent system using LangGraph that includes a supervisor node, multiple specialized nodes, and a validation mechanism with feedback loops.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "### 1. Supervisor Node\n",
    "- **Purpose**: Central coordinator that makes routing decisions\n",
    "- **Responsibility**: Determines which node to call next based on the current state and requirements\n",
    "- **Decision Logic**: Routes requests to appropriate specialized nodes\n",
    "\n",
    "### 2. Router Function\n",
    "- **Purpose**: Implements the routing logic for the supervisor\n",
    "- **Functionality**: Analyzes input and context to decide the next step\n",
    "- **Integration**: Works closely with the supervisor node\n",
    "\n",
    "### 3. Specialized Nodes\n",
    "\n",
    "#### 3.1 LLM Node\n",
    "- **Purpose**: Handles language model calls for text generation\n",
    "- **Functionality**: Processes natural language queries and generates responses\n",
    "- **Use Cases**: Text completion, question answering, content generation\n",
    "\n",
    "#### 3.2 RAG Node (Retrieval-Augmented Generation)\n",
    "- **Purpose**: Combines retrieval and generation for enhanced responses\n",
    "- **Functionality**: Retrieves relevant information from knowledge base and generates contextual responses\n",
    "- **Components**: Document retriever + LLM generator\n",
    "\n",
    "#### 3.3 Web Crawler Node\n",
    "- **Purpose**: Fetches real-time information from the internet\n",
    "- **Functionality**: Performs web searches, scrapes content, and extracts relevant data\n",
    "- **Use Cases**: Current events, real-time data, up-to-date information\n",
    "\n",
    "### 4. Validation Node\n",
    "- **Purpose**: Validates the quality and accuracy of generated outputs\n",
    "- **Validation Methods**:\n",
    "  - Content relevance checking\n",
    "  - Factual accuracy verification\n",
    "  - Format and structure validation\n",
    "  - Completeness assessment\n",
    "\n",
    "### 5. Feedback Loop Mechanism\n",
    "- **Flow**: If validation fails ‚Üí Return to supervisor node\n",
    "- **Decision**: Supervisor determines corrective action (retry with same node or switch to different node)\n",
    "- **Iteration**: Process continues until validation passes\n",
    "\n",
    "### 6. Final Output Generation\n",
    "- **Condition**: Only generates final output after successful validation\n",
    "- **Quality Assurance**: Ensures all outputs meet quality standards\n",
    "\n",
    "## Workflow\n",
    "```\n",
    "Input ‚Üí Supervisor Node ‚Üí Router Function ‚Üí Specialized Node (LLM/RAG/Web Crawler)\n",
    "                ‚Üë                                           ‚Üì\n",
    "                ‚Üê‚Üê‚Üê‚Üê‚Üê‚Üê‚Üê Validation Fails ‚Üê‚Üê‚Üê‚Üê‚Üê Validation Node\n",
    "                                                          ‚Üì\n",
    "                                              Validation Passes\n",
    "                                                          ‚Üì\n",
    "                                                   Final Output\n",
    "```\n",
    "\n",
    "## Implementation Goals\n",
    "1. Build a robust supervisor-agent architecture\n",
    "2. Implement intelligent routing between different processing nodes\n",
    "3. Create specialized nodes for different types of information processing\n",
    "4. Develop comprehensive validation mechanisms\n",
    "5. Establish effective feedback loops for quality control\n",
    "6. Ensure reliable final output generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76111a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all the packages needed for the code to run\n",
    "import os\n",
    "import json\n",
    "import operator\n",
    "from typing import List\n",
    "from pydantic import BaseModel , Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph,END\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader, DirectoryLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9c08866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_rag_system(data_directory: str, chunk_size: int = 200, chunk_overlap: int = 50, k: int = 3):\n",
    "    \"\"\"\n",
    "    Set up a complete RAG (Retrieval-Augmented Generation) system.\n",
    "    \n",
    "    Args:\n",
    "        data_directory (str): Path to the directory containing text files\n",
    "        chunk_size (int): Size of text chunks for splitting documents\n",
    "        chunk_overlap (int): Overlap between chunks\n",
    "        k (int): Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (model, retriever) - Configured model and retriever\n",
    "    \"\"\"\n",
    "    # Config the model\n",
    "    model = ChatGoogleGenerativeAI(model='gemini-1.5-flash')\n",
    "    \n",
    "    # Config the embedding model\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-small-en\")\n",
    "    \n",
    "    # Load documents\n",
    "    loader = DirectoryLoader(\n",
    "        data_directory, \n",
    "        glob=\"./*.txt\", \n",
    "        loader_cls=TextLoader\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    \n",
    "    # Split documents\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    new_docs = text_splitter.split_documents(documents=docs)\n",
    "    \n",
    "    # Create vector database\n",
    "    db = Chroma.from_documents(new_docs, embeddings)\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "    \n",
    "    return model, retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fefbe2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicSelectionParser(BaseModel):\n",
    "    \"\"\"Parser for topic selection with reasoning.\"\"\"\n",
    "    Topic: str = Field(description=\"selected topic\")\n",
    "    Reasoning: str = Field(description='Reasoning behind topic selection')\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    \"\"\"Agent state for StateGraph management.\"\"\"\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    validation_status: str  # \"pending\", \"passed\", \"failed\"\n",
    "    next_action: str       # \"supervisor\", \"final_output\"\n",
    "\n",
    "def create_topic_selection_system():\n",
    "    \"\"\"\n",
    "    Create a topic selection parser and agent state for the LangGraph system.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (parser, AgentState, format_instructions)\n",
    "    \"\"\"\n",
    "    # Create parser\n",
    "    parser = PydanticOutputParser(pydantic_object=TopicSelectionParser)\n",
    "    \n",
    "    # Get format instructions\n",
    "    format_instructions = parser.get_format_instructions()\n",
    "    \n",
    "    return parser, AgentState, format_instructions\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"\n",
    "    Format documents by joining their page content.\n",
    "    \n",
    "    Args:\n",
    "        docs: List of document objects\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted document content\n",
    "    \"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb9b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(state:AgentState):\n",
    "    \n",
    "    question=state[\"messages\"][-1]\n",
    "    \n",
    "    print(\"Question\",question)\n",
    "    \n",
    "    template=\"\"\"\n",
    "    Your task is to classify the given user query into one of the following categories: [USA,Not Related, Websearch]. \n",
    "    Only respond with the category name and nothing else.\n",
    "\n",
    "    User query: {question}\n",
    "    {format_instructions}\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt= PromptTemplate(\n",
    "        template=template,\n",
    "        input_variable=[\"question\"],\n",
    "        partial_variables={\"format_instructions\": parser.get_format_instructions()}\n",
    "    )\n",
    "    \n",
    "    \n",
    "    chain= prompt | model | parser\n",
    "    \n",
    "    response = chain.invoke({\"question\":question})\n",
    "    \n",
    "    print(\"Parsed response:\", response)\n",
    "    \n",
    "    return {\"messages\": [response.Topic]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d55810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_router(state: AgentState):\n",
    "    \"\"\"Enhanced router that handles validation flow\"\"\"\n",
    "    \n",
    "    # Check if we have validation status\n",
    "    if \"validation_status\" in state:\n",
    "        if state[\"validation_status\"] == \"failed\":\n",
    "            print(\"üîÑ Validation failed - routing back to supervisor\")\n",
    "            return \"supervisor\"\n",
    "        elif state[\"validation_status\"] == \"passed\":\n",
    "            print(\"‚úÖ Validation passed - generating final output\")\n",
    "            return \"final_output\"\n",
    "    \n",
    "    # Original routing logic\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if isinstance(last_message, dict):\n",
    "        last_message = last_message.get(\"content\", \"\")\n",
    "    \n",
    "    if \"supervisor\" in str(last_message).lower():\n",
    "        return \"supervisor\"\n",
    "    elif any(keyword in str(last_message).lower() for keyword in [\"rag\", \"document\", \"file\"]):\n",
    "        return \"rag\"\n",
    "    elif any(keyword in str(last_message).lower() for keyword in [\"web\", \"search\", \"latest\", \"news\"]):\n",
    "        return \"webcrawl\"\n",
    "    elif any(keyword in str(last_message).lower() for keyword in [\"validation\", \"validate\"]):\n",
    "        return \"validation\"\n",
    "    else:\n",
    "        return \"llm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8aec583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def router(state:AgentState):\n",
    "    print(\"-> ROUTER ->\")\n",
    "    \n",
    "    last_message=state[\"messages\"][-1]\n",
    "    print(\"last_message:\", last_message)\n",
    "    \n",
    "    if \"usa\" in last_message.lower():\n",
    "        return \"RAG Call\"\n",
    "    elif \"not related\" in last_message.lower():\n",
    "        return \"LLM Call\"\n",
    "    else:\n",
    "        return \"WEBCRAWL Call\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d9186ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG Function\n",
    "def function_2(state:AgentState):\n",
    "    print(\"-> RAG Call ->\")\n",
    "    \n",
    "    question = state[\"messages\"][0]\n",
    "    \n",
    "    prompt=PromptTemplate(\n",
    "        template=\"\"\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\"\",\n",
    "        \n",
    "        input_variables=['context', 'question']\n",
    "    )\n",
    "    \n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    result = rag_chain.invoke(question)\n",
    "    return  {\"messages\": [result]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba06e0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM Function\n",
    "def function_3(state:AgentState):\n",
    "    print(\"-> LLM Call ->\")\n",
    "    question = state[\"messages\"][0]\n",
    "    \n",
    "    # Normal LLM call\n",
    "    complete_query = \"Anwer the follow question with you knowledge of the real world. Following is the user question: \" + question\n",
    "    response = model.invoke(complete_query)\n",
    "    return {\"messages\": [response.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab3764a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WebCrawling Function\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "API_KEY=os.getenv(\"TAVILY_API_KEY\")\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "def function_4(state:AgentState):\n",
    "    print(\"-> WEBCRAWL Call ->\")\n",
    "    question = state[\"messages\"][0]\n",
    "\n",
    "    tool=TavilySearchResults(tavily_api_key=API_KEY)\n",
    "    \n",
    "    # Normal LLM call\n",
    "    complete_query = \"Anwer the follow question with you knowledge of the real world. Following is the user question: \" + question\n",
    "    response = tool.invoke({\"query\": complete_query})\n",
    "    return {\"messages\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60d81bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced function_4 with clickable links\n",
    "def function_4(state: AgentState):\n",
    "    print(\"-> Web Search & Summarization Started\")\n",
    "    \n",
    "    # Get the question from state\n",
    "    if isinstance(state[\"messages\"], list) and len(state[\"messages\"]) > 0:\n",
    "        question = state[\"messages\"][-1]\n",
    "        print(\"Last message in state:\", question)\n",
    "        if isinstance(question, dict):\n",
    "            question = question.get(\"content\", \"\")\n",
    "    else:\n",
    "        question = str(state[\"messages\"])\n",
    "    \n",
    "    try:\n",
    "        # Search the web\n",
    "        tool = TavilySearchResults(tavily_api_key=API_KEY, max_results=3)\n",
    "        results = tool.invoke({\"query\": question})\n",
    "        \n",
    "        if results and len(results) > 0:\n",
    "            # Collect content from multiple sources\n",
    "            all_content = []\n",
    "            sources = []\n",
    "            \n",
    "            for i, result in enumerate(results[:3]):  # Top 3 results\n",
    "                content = result.get('content', '')\n",
    "                title = result.get('title', f'Source {i+1}')\n",
    "                url = result.get('url', '')\n",
    "                \n",
    "                if content:\n",
    "                    all_content.append(content)\n",
    "                    # Include clickable links in sources\n",
    "                    if url:\n",
    "                        sources.append(f\"‚Ä¢ [{title}]({url})\")\n",
    "                    else:\n",
    "                        sources.append(f\"‚Ä¢ {title}\")\n",
    "            \n",
    "            # Combine and summarize content\n",
    "            combined_content = \" \".join(all_content)\n",
    "            summary = summarize_content(combined_content, question)\n",
    "            \n",
    "            # Format final answer with clickable links\n",
    "            answer = f\"\"\"üì∞ **Web Search Summary**\n",
    "\n",
    "**Question:** {question}\n",
    "\n",
    "**Summary:**\n",
    "{summary}\n",
    "\n",
    "**Sources with Links:**\n",
    "{chr(10).join(sources[:3])}\n",
    "\n",
    "---\n",
    "*Search completed at {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*\n",
    "*Click on source titles to visit the original articles*\"\"\"\n",
    "            \n",
    "        else:\n",
    "            answer = \"‚ùå No search results found for your query.\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        answer = f\"üö® Search error: {str(e)}\"\n",
    "    \n",
    "    print(\"-> Web Search & Summarization Complete\")\n",
    "    # Ensure the result is formatted as markdown\n",
    "    #print(answer)\n",
    "    # Return in the expected format for your agent state\n",
    "    return {\"messages\": state[\"messages\"] + [answer]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b372ffa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test the web search function\n",
    "# state = {\"messages\": [\"find the latest news about AI technology\"]}\n",
    "# print(\"Testing web search function...\")\n",
    "# print(\"Input state:\", state)\n",
    "# print(\"-\" * 50)\n",
    "\n",
    "# result = function_4(state)\n",
    "# print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73a8b94f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_enhanced_langgraph_workflow(\n",
    "    agent_state_class,\n",
    "    supervisor_func,\n",
    "    rag_func,\n",
    "    llm_func,\n",
    "    webcrawl_func,\n",
    "    validation_func,\n",
    "    final_output_func,\n",
    "    router_func\n",
    "):\n",
    "    workflow = StateGraph(agent_state_class)\n",
    "    \n",
    "    # Add all nodes\n",
    "    workflow.add_node(\"supervisor\", supervisor_func)\n",
    "    workflow.add_node(\"rag\", rag_func)\n",
    "    workflow.add_node(\"llm\", llm_func)\n",
    "    workflow.add_node(\"webcrawl\", webcrawl_func)\n",
    "    workflow.add_node(\"validation\", validation_func)\n",
    "    workflow.add_node(\"final_output\", final_output_func)\n",
    "    \n",
    "    # Define the flow\n",
    "    workflow.set_entry_point(\"supervisor\")\n",
    "    \n",
    "    # From supervisor to task nodes\n",
    "    workflow.add_conditional_edges(\n",
    "        \"supervisor\",\n",
    "        router_func,\n",
    "        {\n",
    "            \"rag\": \"rag\",\n",
    "            \"llm\": \"llm\", \n",
    "            \"webcrawl\": \"webcrawl\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # All task nodes go to validation\n",
    "    workflow.add_edge(\"rag\", \"validation\")\n",
    "    workflow.add_edge(\"llm\", \"validation\")\n",
    "    workflow.add_edge(\"webcrawl\", \"validation\")\n",
    "    \n",
    "    # From validation - conditional routing\n",
    "    workflow.add_conditional_edges(\n",
    "        \"validation\",\n",
    "        lambda state: state.get(\"next_action\", \"supervisor\"),\n",
    "        {\n",
    "            \"supervisor\": \"supervisor\",  # If validation failed\n",
    "            \"final_output\": \"final_output\"  # If validation passed\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Final output ends the workflow\n",
    "    workflow.add_edge(\"final_output\", END)\n",
    "    \n",
    "    return workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "403b908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_langgraph_workflow(agent_state_class, supervisor_func, rag_func, llm_func, router_func, webcrawl_func):\n",
    "    \"\"\"\n",
    "    Create a modular LangGraph workflow with supervisor, RAG, and LLM nodes.\n",
    "    \n",
    "    Args:\n",
    "        agent_state_class: The AgentState class for state management\n",
    "        supervisor_func: Function for the supervisor node\n",
    "        rag_func: Function for the RAG node\n",
    "        llm_func: Function for the LLM node\n",
    "        router_func: Router function for conditional edges\n",
    "        webcrawl_func: Function for the web crawling node\n",
    "    \n",
    "    Returns:\n",
    "        compiled workflow app\n",
    "    \"\"\"\n",
    "    # Initialize workflow\n",
    "    workflow = StateGraph(agent_state_class)\n",
    "    \n",
    "    # Add nodes\n",
    "    workflow.add_node(\"Supervisor\", supervisor_func)\n",
    "    workflow.add_node(\"RAG\", rag_func)\n",
    "    workflow.add_node(\"LLM\", llm_func)\n",
    "    workflow.add_node(\"WEBCRAWL\", webcrawl_func)\n",
    "    \n",
    "    # Set entry point\n",
    "    workflow.set_entry_point(\"Supervisor\")\n",
    "    \n",
    "    # Add conditional edges from supervisor\n",
    "    workflow.add_conditional_edges(\n",
    "        \"Supervisor\",\n",
    "        router_func,\n",
    "        {\n",
    "            \"RAG Call\": \"RAG\",\n",
    "            \"LLM Call\": \"LLM\",\n",
    "            \"WEBCRAWL Call\": \"WEBCRAWL\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Add terminal edges\n",
    "    workflow.add_edge(\"RAG\", END)\n",
    "    workflow.add_edge(\"LLM\", END)\n",
    "    workflow.add_edge(\"WEBCRAWL\", END)\n",
    "    \n",
    "    # Compile and return the workflow\n",
    "    app = workflow.compile()\n",
    "    return app\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1cd2ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_5_validation(state: AgentState):\n",
    "    \"\"\"Validation function to check the quality of generated output\"\"\"\n",
    "    print(\"-> VALIDATION Started\")\n",
    "    \n",
    "    # Get the last message (the output to validate)\n",
    "    if isinstance(state[\"messages\"], list) and len(state[\"messages\"]) > 0:\n",
    "        last_message = state[\"messages\"][-1]\n",
    "        if isinstance(last_message, dict):\n",
    "            content = last_message.get(\"content\", \"\")\n",
    "        else:\n",
    "            content = str(last_message)\n",
    "    else:\n",
    "        content = str(state[\"messages\"])\n",
    "    \n",
    "    # Get the original question (first message)\n",
    "    original_question = \"\"\n",
    "    if isinstance(state[\"messages\"], list) and len(state[\"messages\"]) > 0:\n",
    "        first_message = state[\"messages\"][0]\n",
    "        if isinstance(first_message, dict):\n",
    "            original_question = first_message.get(\"content\", \"\")\n",
    "        else:\n",
    "            original_question = str(first_message)\n",
    "    \n",
    "    print(f\"Validating response for: {original_question[:50]}...\")\n",
    "    \n",
    "    # Validation criteria\n",
    "    validation_results = validate_response(content, original_question)\n",
    "    \n",
    "    if validation_results[\"is_valid\"]:\n",
    "        print(\"‚úÖ Validation PASSED\")\n",
    "        # Add validation stamp to the response\n",
    "        validated_response = f\"\"\"\n",
    "{content}\n",
    "\n",
    "---\n",
    "‚úÖ **Validation Status:** PASSED\n",
    "üìä **Quality Score:** {validation_results['score']}/10\n",
    "‚è∞ **Validated at:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "        return {\n",
    "            \"messages\": state[\"messages\"][:-1] + [validated_response],\n",
    "            \"validation_status\": \"passed\",\n",
    "            \"next_action\": \"final_output\"\n",
    "        }\n",
    "    else:\n",
    "        print(\"‚ùå Validation FAILED\")\n",
    "        # Add validation failure message\n",
    "        failure_message = f\"\"\"\n",
    "‚ùå **Validation Failed**\n",
    "\n",
    "**Issues Found:**\n",
    "{chr(10).join(f'‚Ä¢ {issue}' for issue in validation_results['issues'])}\n",
    "\n",
    "**Score:** {validation_results['score']}/10\n",
    "**Recommendation:** {validation_results['recommendation']}\n",
    "\"\"\"\n",
    "        return {\n",
    "            \"messages\": state[\"messages\"] + [failure_message],\n",
    "            \"validation_status\": \"failed\",\n",
    "            \"next_action\": \"supervisor\"\n",
    "        }\n",
    "\n",
    "def validate_response(content: str, question: str) -> dict:\n",
    "    \"\"\"Validate the response quality\"\"\"\n",
    "    issues = []\n",
    "    score = 10\n",
    "    \n",
    "    # Check 1: Content length\n",
    "    if len(content) < 50:\n",
    "        issues.append(\"Response too short\")\n",
    "        score -= 3\n",
    "    \n",
    "    # Check 2: Contains error messages\n",
    "    error_keywords = [\"error\", \"failed\", \"could not\", \"unable to\", \"no results\"]\n",
    "    if any(keyword in content.lower() for keyword in error_keywords):\n",
    "        issues.append(\"Contains error messages\")\n",
    "        score -= 2\n",
    "    \n",
    "    # Check 3: Relevance to question\n",
    "    question_words = set(question.lower().split())\n",
    "    content_words = set(content.lower().split())\n",
    "    relevance = len(question_words.intersection(content_words)) / len(question_words)\n",
    "    \n",
    "    if relevance < 0.2:\n",
    "        issues.append(\"Low relevance to original question\")\n",
    "        score -= 3\n",
    "    \n",
    "    # Check 4: Has proper formatting\n",
    "    if \"**\" not in content and \"#\" not in content:\n",
    "        issues.append(\"Poor formatting\")\n",
    "        score -= 1\n",
    "    \n",
    "    # Check 5: Contains sources (for web search)\n",
    "    if \"web search\" in question.lower() or \"latest\" in question.lower():\n",
    "        if \"source\" not in content.lower() and \"http\" not in content.lower():\n",
    "            issues.append(\"Missing sources for web search\")\n",
    "            score -= 2\n",
    "    \n",
    "    # Determine recommendation\n",
    "    if score >= 7:\n",
    "        recommendation = \"Response quality is good\"\n",
    "    elif score >= 5:\n",
    "        recommendation = \"Response needs minor improvements\"\n",
    "    else:\n",
    "        recommendation = \"Response needs significant improvement - retry with different approach\"\n",
    "    \n",
    "    return {\n",
    "        \"is_valid\": score >= 6,  # Pass threshold\n",
    "        \"score\": max(0, score),\n",
    "        \"issues\": issues,\n",
    "        \"recommendation\": recommendation\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0903f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_6_final_output(state: AgentState):\n",
    "    \"\"\"Generate the final polished output\"\"\"\n",
    "    print(\"-> FINAL OUTPUT Generation\")\n",
    "    \n",
    "    # Get the validated response\n",
    "    validated_response = state[\"messages\"][-1]\n",
    "    \n",
    "    final_output = f\"\"\"\n",
    "# üéØ Final AI Agent Response\n",
    "\n",
    "{validated_response}\n",
    "\n",
    "---\n",
    "ü§ñ **Generated by:** Multi-Agent LangGraph System\n",
    "üîç **Processing Steps:** Supervisor ‚Üí Task Execution ‚Üí Validation ‚Üí Final Output\n",
    "‚úÖ **Quality Assured:** Response validated and approved\n",
    "\"\"\"\n",
    "    \n",
    "    print(\"‚úÖ Final output generated successfully!\")\n",
    "    \n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [final_output],\n",
    "        \"validation_status\": \"completed\",\n",
    "        \"next_action\": \"end\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e29d30f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kausik/Desktop/Agentic-AI/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "model, retriever = setup_rag_system(\"/Users/kausik/Desktop/Agentic-AI/3-langgraph/data2\")\n",
    "parser, AgentState, format_instructions = create_topic_selection_system()\n",
    "app = create_langgraph_workflow(\n",
    "    agent_state_class=AgentState,\n",
    "    supervisor_func=function_1,\n",
    "    rag_func=function_2,\n",
    "    llm_func=function_3,\n",
    "    webcrawl_func=function_4,\n",
    "    router_func=router\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a582f2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the enhanced app with validation\n",
    "import datetime\n",
    "enhanced_app = create_enhanced_langgraph_workflow(\n",
    "    agent_state_class=AgentState,\n",
    "    supervisor_func=function_1,\n",
    "    rag_func=function_2,\n",
    "    llm_func=function_3,\n",
    "    webcrawl_func=function_4,\n",
    "    validation_func=function_5_validation,\n",
    "    final_output_func=function_6_final_output,\n",
    "    router_func=enhanced_router\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ee49c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question web search can you tell me the industrial growth of world's most powerful economy?\n",
      "Parsed response: Topic='USA' Reasoning=\"The query asks about the industrial growth of the world's most powerful economy, which is generally considered to be the USA.\"\n",
      "-> LLM Call ->\n",
      "-> VALIDATION Started\n",
      "Validating response for: web search can you tell me the industrial growth o...\n",
      "‚úÖ Validation PASSED\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'datetime' has no attribute 'now'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test with validation\u001b[39;00m\n\u001b[32m      2\u001b[39m state = {\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[33m\"\u001b[39m\u001b[33mweb search can you tell me the industrial growth of world\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms most powerful economy?\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      4\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvalidation_status\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpending\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mnext_action\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msupervisor\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m }\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m result = \u001b[43menhanced_app\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFinal result:\u001b[39m\u001b[33m\"\u001b[39m, result)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2719\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, **kwargs)\u001b[39m\n\u001b[32m   2716\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any], Any]] = []\n\u001b[32m   2717\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m2719\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2720\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2721\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2722\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2723\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2724\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2725\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_during\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2728\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2729\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2730\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   2731\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2732\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2733\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mints\u001b[49m\u001b[43m \u001b[49m\u001b[43m:=\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINTERRUPT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m   2734\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Agentic-AI/venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2436\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2434\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2435\u001b[39m             loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2443\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mfunction_5_validation\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     30\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Validation PASSED\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m         \u001b[38;5;66;03m# Add validation stamp to the response\u001b[39;00m\n\u001b[32m     32\u001b[39m         validated_response = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;132;01m{\u001b[39;00mcontent\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     34\u001b[39m \n\u001b[32m     35\u001b[39m \u001b[33m---\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[33m‚úÖ **Validation Status:** PASSED\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[33müìä **Quality Score:** \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalidation_results[\u001b[33m'\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/10\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[33m‚è∞ **Validated at:** \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdatetime\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnow\u001b[49m().strftime(\u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     40\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     41\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: state[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m][:-\u001b[32m1\u001b[39m] + [validated_response],\n\u001b[32m     42\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mvalidation_status\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mpassed\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnext_action\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfinal_output\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     44\u001b[39m         }\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mAttributeError\u001b[39m: module 'datetime' has no attribute 'now'",
      "During task with name 'validation' and id '14831727-a8da-fafb-081a-e8d153b542cb'"
     ]
    }
   ],
   "source": [
    "# Test with validation\n",
    "state = {\n",
    "    \"messages\": [\"web search can you tell me the industrial growth of world's most powerful economy?\"],\n",
    "    \"validation_status\": \"pending\",\n",
    "    \"next_action\": \"supervisor\"\n",
    "}\n",
    "\n",
    "result = enhanced_app.invoke(state)\n",
    "print(\"Final result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f76177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install graphviz\n",
    "# !pip install pygraphviz\n",
    "\n",
    "# Then use this for best results:\n",
    "from IPython.display import Image, display\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cef4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different queries\n",
    "state1 = {\"messages\": [\"latest AI technology news\"]}\n",
    "result1 = app.invoke(state1)\n",
    "\n",
    "# state2 = {\"messages\": [\"current stock market updates\"]}\n",
    "# result2 = app.invoke(state2)\n",
    "\n",
    "# state3 = {\"messages\": [\"recent climate change developments\"]}\n",
    "# result3 = app.invoke(state3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a76eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "state={\"messages\":[\"what is a gdp of usa in web?\"]}\n",
    "\n",
    "app.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5ce636",
   "metadata": {},
   "outputs": [],
   "source": [
    "state={\"messages\":[\"web search can you tell me the industrial growth of world's most powerful economy search in web?\"]}\n",
    "app.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636fb705",
   "metadata": {},
   "outputs": [],
   "source": [
    "state={\"messages\":[\"can you tell me the industrial growth of world's poor economy?\"]}\n",
    "result=app.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d60f7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "state={\"messages\":[\"What's the capital of India from web?\"]}\n",
    "result=app.invoke(state)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
